分类任务就是确定对象属于哪个**预定义**的**目标类**

1. 模型的过分拟合
2. 评估和比较分类技术性能的方法

分类任务输入的是 **记录 **的集合

记录也称实例或者样例

元组（x，y）

x表示样例的**各种属性**，是属性的集合

y表示特殊的属性，指出样例的**类标号**（分类属性或目标属性）

==属性集可以是离散的也可以是连续的，但是类标号必须是离散属性==

==回归则是连续的属性==

离散理解：目标属性所在的集合是有限的，分类问题就是根据属性集来将对象分到有限的目标属性集合中



分类：分类任务就是通过学习得到一个**目标函数**，通过这个函数把每个属性集x映射到一个预先定义的类标号y中

分类模型可以用于描述性模型和预测性模型

描述性模型并不直接预测或分类数据，而是提供有关数据分布和属性的相关统计信息。它们是数据探索和数据分析的工具，有助于发现数据集中的模式、异常值和相关性。

预测性模型依赖于训练数据集来学习特征之间的关系，并使用这些关系在新数据上进行分类。这些模型使用训练集中的输入特征和已知的目标属性来构建一个模型，然后应用于未知目标属性的新样本。

描述性模型旨在描述数据的特征和属性，帮助我们理解数据集本身。预测性模型则是通过学习样本之间的关系来预测目标属性的类别。

分类适合二元或者标称类型的数据集
标称型：一般在有限的数据中取，而且结果只存在于预定义中的类标号。 标称数据无序。



分类法包括**决策树分析法**，**基于规则的分析法**，**神经网络**，**支持向量机**和**朴素贝叶斯分类法**。

根据模型正确和错误预测的检验记录计数进行评估

这些计数存放与**混淆矩阵**中

![image-20230915141418747](C:\Users\Pluto\AppData\Roaming\Typora\typora-user-images\image-20230915141418747.png)

准确率和错误率

准确率表示正确预估数除以预测总数

错误率表示错误预测数除以预测总数

评估模型分类标准那就是尽可能高的准确率或尽可能低的错误率



# 决策树分类法

通过“提出”一系列精心构思的关于检验记录属性的问题，可以解决分类问题。

决策树：有结点和有向边组成的层次结构。

根节点：有零条或多条出边

内部结点：内部结点：一条入边和两条或多条出边（如果没有出边或者只有一条出边就不算内部结点）

叶节点/终结点：只有一条入边，没有出边（类标号）

次最优决策树，因为根据属性的规模来构造决策树那么搜索量级是指数级别，所以寻找次最优决策树

决策树的规模指的是叶节点的个数，n^k

## Hunt算法

==贪心算法，局部最优==

把训练记录**划分为较纯的子集**，以递归方式建立决策树

D是与结点t有关的训练记录集，当D中的记录包括多个类标号时，就要使用属性测试条件对D进行划分。然后根据结果创建子女结点，将记录放入，然后重复知道D中只包含一个类标号，这个结点就变成叶节点。

![image-20230915150914992](C:\Users\Pluto\AppData\Roaming\Typora\typora-user-images\image-20230915150914992.png)

这里为空时选择类标号多数类是因为统计学的原理，多数类出现的概率更大，即使这个属性为空，但这个类标号出现的概率依然是最大的，这样模型也就准确率越高。

![image-20230915151223048](C:\Users\Pluto\AppData\Roaming\Typora\typora-user-images\image-20230915151223048.png)

在属性集无法进一步划分记录的时候，直接将D关联的结点t设置为叶节点，类标号直接设置为D中类标号的多数类，尽管里面可能出现不同的类标号（目标属性），但是直接设置为多数类。



停止生长，可以通过所有记录都有相同的属性值或类都是相同类来停止。

k个属性值的二元划分是指k-1个属性都能有两种可能，但是最后一个属性只能在一个集合中

我们的目标是找到能够正确预测或分离数据的特征或属性。因此，通常会选择那些能够最大程度地减少不纯性的划分策略。

**子结点不纯度越低越好**







先验假设



#### 模型的过分拟合

**误差分为训练误差和泛化误差**

**训练误差是在训练记录就有的误差**

**泛化误差是面对未知样本产生的误差的期望**，这个不是实际测试出来的，而是估计出来的



噪声，错误数据，或者不需要的干扰数据，可能导致过度拟合



过分拟合原因
**噪声和例外导致，缺乏代表性样本导致**

**缺乏代表性样本，即缺少样本，只根据少量样本来划分属性，导致过分拟合**

**多重比较法导致的过分拟合**



#### 1估计泛化误差

直接使用训练误差（再代入误差），假设训练记录很有代表性，实际上，这没有道理，过拟合的前提下，这两者完全不等



==奥卡姆剃刀原则/节俭原则==

**（减小过拟合的可能性）**

**给定两个具有相同泛化误差的模型，较简单的模型比较复杂的模型更加可取。**

#### 2增加复杂度来估计泛化误差

悲观误差评估：使用训练误差和复杂度罚项来计算泛化误差

计算公式

 ![image-20230918232805189](C:\Users\Pluto\AppData\Roaming\Typora\typora-user-images\image-20230918232805189.png)

![image-20230918232817389](C:\Users\Pluto\AppData\Roaming\Typora\typora-user-images\image-20230918232817389.png)

**总训练误差，对于决策树的训练误差是总训练误差/记录个数**

**所以总训练误差就是判断错的个数**

**加上每个结点的罚项，注意是叶节点的个数乘罚项**

**下面就是训练记录数**



整合起来就是，悲观误差估计：（判断错误的个数+最终叶节点个数*罚项）/记录个数

==悲观误差估计的泛化误差都会比训练误差要大== 用上限估计下限

#### 最小描述长度原则



#### 3估计统计上界

不是很懂

#### 4利用确认集

在记录中一部分建立模型，一部分测试模型



### 处理过分拟合

剪枝

**先剪枝**

提前终止规则

**当观察到不纯性度量的增益或者估计的泛化误差的改进低于某个阙值就停止扩展叶节点**

估计泛化误差上面有几种方法，比如直接用训练误差（再代入误差）；结合复杂度，加入罚项；估计统计上线；使用一部分确认集来测试。

但是，先剪枝很难找到这个阙值，如果阙值太高那么就很难拟合完全，太低就容易过拟合

除此之外，就算不纯性度量增益很低，也可能这是一颗很好的决策树。

**后剪枝**

==首先不进行剪枝操作，直接构建数，后面进行剪枝，将子树用叶节点代替，叶节点的类标号就用子树中最多的类标号来代替。==

或者

用子树中最常使用的分支代替子树



### 基于规则的分类器

规则评估

+ 似然比评估
+ Laplace度量
+ FOIL信息增益度量

规则剪枝条件，通过函数先得到规则，然后进行剪枝，如果泛化误差更小，或者是悲观估计误差更小，那么剪枝

==规则提取的直接算法==

顺序覆盖 RIPPER算法

#####  RIPPER算法

RIPPER算法针对两类问题，选择多数类为默认类，得到少数类的规则

针对多类问题，按照类的频率排序，少的排前面，先给少的总结规则，最后的作为默认类

RIPPER算法使用一般到特殊的策略，采用FOIL信息增益度量来判断是否添加合取项，当规则开始覆盖反例时，停止添加合取项，然后对规则进行剪枝，剪枝方法根据正例-反例/正例+反例

如果这个度量变大了，那么说明反例多了，正例少了，不能添加这个合取项，从最后的合取项开始剪枝，如果不符合，再看前面的合取项

==规则提取的间接方法==

终止条件，如果规则满足条件，（最小马描述长度原则）,也就是规则集的总描述长度增加要小于一个定值，那么有效，否则不添加进入，还有一个终止条件是规则在确认集上的准确率要大于50%

==规则提取的间接方法==

首先得到决策树，然后对于从根节点到叶节点都作为初始规则，然后剪枝，只要悲观估计误差更小，那么剪枝，最后多个规则经过剪枝后可能产生同样的规则，这就要去除相同的规则

然后对于C4.5算法，通过类的规则排序对规则集进行排序，然后计算每个子类的总描述长度，按从小到大排序



RIPPER和C4.5都基于类的规则，不过前者是直接，后者是通过决策树



### KNN最近邻分类器（消极学习方法，根据相似度直接分类）

对于每一个训练样例，都计算与其他训练样例的距离（相似度）

然后选出最相似的K个样例，然后取里面类最多的将其赋给这k个样例。

还可以考虑相似程度，使得相似程度低的样例对整体影响更小一些，1/相似度的平方  做权值然后乘1求和

相关性里面有一个皮尔森相关，这个用均值来计算相关程度。



## 贝叶斯分类器

由于属性集合类变量的关系可能不能确定，所以存在一种对**属性集和类变量的概率关系建模**的方法



先验知识：提前验证的知识。

==贝叶斯定理==   概率论学了

贝叶斯定理就是使用了后验概率，利用经验，得到更加符合实际的概率

那要分类就是在属性集x的经验下，去得到后验概率，看属于哪一个类



要计算在属性集为X的条件下，类为Y的后验概率，需要以下几个东西

+ 先验概率P（Y） 这个可以直接根据训练集中Y的概率算出
+ 证据P（X）表示属性为X的概率，这个是一个常数
+ 类条件概率P（X|Y） 在类为Y的前提下，属性X的概率



其中，证据P（X）是定值，先按概率很容易算出，对于类条件概率的估计，通过两种方法

## 朴素贝叶斯分类器

分类器利用贝叶斯概率，概率之间独立，m估计

## 贝叶斯信念网络

信念网络，创建有向无环图，内附概率表，可以不独立

## 人工神经网络ANN artificial nerve network

树突与轴突的连接点叫做神经键

## 感知器

最简单的模型

感知节点包含几个输入结点表示输入属性，**一个**输出结点，用来提供输出

感知器对输入加权求和，再**减去偏置因子**



==**sign符号函数**==

当里面参数为正时输出1，参数为负时输出-1



在感知器学习算法中，权值的更新会在每个训练样本上进行。以下是对于第一个样例的权值更新过程：

1. 初始化权值：开始时，权值会被初始化为一些初始值，通常为0或一个小的随机数。

2. 计算预测值：对于给定的输入样例，感知器会根据当前权值计算出预测值。这个预测值可以通过将输入样例的特征与权值相乘，再进行求和的方式得到。

3. 更新权值：对于每个权值，根据预测值与样例的真实值之间的差异，进行权值的调整。具体的权值更新规则如下：
   1. 如果预测值等于真实值，则权值保持不变。
   2. 如果预测值大于真实值，意味着预测过高，这时权值需要减小一些，以减小预测值。更新规则为：权值 = 权值 - 学习率 * 输入特征。
   3. 如果预测值小于真实值，意味着预测过低，这时权值需要增加一些，以增加预测值。更新规则为：权值 = 权值 + 学习率 * 输入特征。

4. 重复2和3：对于每一个训练样本，重复步骤2和3，直至达到指定的停止条件（例如达到最大迭代次数或误差小于某个阈值）。

需要注意的是，为了使感知器算法能够收敛，还需要选择合适的学习率和初始权值。学习率控制了权值更新的幅度，而初始权值会影响算法的初始状态。正确选择这两个参数可以提高算法的性能和收敛速度。

这个算法对于每一个样本都进行权值更新操作

感知器可以说是单层神经网络

## 多层神经网络

输入层  1层

隐藏层  n层

输出层  1层

前馈神经网络  只能连到下一层

递归神经网络  可以连本身层或是任意所有上面的层



**集合函数**

激活函数除了符号函数以外，还可以使用其他函数，这就使得输入值和输出值可以为非线性关系

**一个隐藏结点可以看做一个感知器**

但是，不能像计算感知器一样计算隐藏结点的权值，因为，误差项是实际减预测，隐藏结点的预测与实际的关系不知，所以改用另一种方法

### 梯度下降

ANN代价函数

![image-20230922175401389](C:\Users\Pluto\AppData\Roaming\Typora\typora-user-images\image-20230922175401389.png)

需要做的就是最小化这个代价函数

其中yi是w.x

##### 反向传播

先前向传播，计算出每个结点的具体值，然后计算出代价函数

然后后向传播，梯度下降，直到收敛或者达到最大迭代次数

## 支持向量机SVM

训练实例的一个子集表示决策边界：支持向量

最大边缘超平面，就是平移决策边界，知道接触到不同的类，这两个边之间的距离就是边缘

我们要最大的

因为最大边缘超平面具有更好的泛化误差（统计学原理）

决策边界

WX+b = 0

其中W，X都是向量，可以理解为X为各个属性，W表示属性的权重，b则是一个辅助向量，对平面进行微调

**其中W垂直与决策边界**

要想最大化边缘，首先知道边缘公式

d = 2 / || w||

其中下面表示二范数开根号

然后要最大化它等价于最小化||w||²/ 2

||w||²/ 2 就是目标函数   w二范数/2

约束条件即为，分类正确



将其转化为拉格朗日函数，就是d - 约束条件*拉格朗日乘子求和

这里有点晕

先看结论吧



首先构造拉格朗日函数，但是由于不知道拉格朗日乘子，无法求解W，b

因此，构造对偶拉格朗日函数Ld  算出一组拉格朗日乘子

然后带回公式计算B，在带入公式计算b，然后**线性决策边界**就构造完成



### 不可分情况

利用软边缘构造允许一定训练错误的决策边界

引入松弛变量，然后需要惩罚错误分类项





# 聚类

==聚类分析将数据划分成**有意义**或**有用**的组（簇）==

+ 将对象划分成簇的集合的各种方法

+ 聚类的不同类型

三种专门的聚类技术

+ K均值
+ 凝聚的层次聚类
+ DBSCAN

评估 聚类算法产生簇 的方法



聚类分析：聚类分析仅根据在数据中发现的描述对象及其关系的信息，将数据分组。

组内相似性（同质性）越大，组间差别越大，聚类就越好。

有监督学习和无监督学习的区别就在于有没有原始训练数据

一般来说，对于聚类算法，不需要显式的训练模型



**聚类将数据按照相似性（同质性）或者距离度量来赋予它们类标号**



==分类和聚类不同==

### 聚类类型

**划分聚类（非嵌套聚类）**

简单地将数据对象集划分成不重叠的子集

**层次聚类（嵌套聚类）**

允许簇拥有子簇

**互斥的**

每个对象都指派到单个类  一个对象不会分到多个类中

**重叠的**

一个对象可以放在多个类中

**模糊聚类**

设立一个隶属值表示概率属于各个类

**完全聚类**

所有对象都进行聚类

**不完全聚类**

由于一些噪声或离群点或”不感兴趣的背景“，一些对象不需要聚类



### 不同簇类型

**明显分离的簇：**

每个点到同簇中任意点的距离比到不同簇中所有点的距离更近

**基于中心的簇：**

每个点到道其簇中心的距离比到其他簇中心的距离更近

**基于图的簇：**

将簇理解为连通分支，其他都不连通，可能被噪声影响，将两个本该划分为不同簇的对象，连接成一个簇

**基于密度的簇：**

根据对象的稠密程度来划分簇，这种方法可以很好解决噪声和离群点

**基于性质的簇：**

就是根据性质，性质这种东西不同情况需要的性质也不同，包括了前面所有簇的类型

## 聚类技术

### K均值

K均值是**基于原型的，划分的**聚类

选取K个质心

然后迭代聚类

终止条件为点都不改变簇，或者弱化，只有1%的点改变簇



**相似性度量**

==欧氏距离，但是有时候可以通过二分K均值来减少计算量==



目标函数：最小化对象到其簇质心的L2距离的平方和

**时间复杂度和空间复杂度**

时间复杂度不大

空间复杂度也不大（可以说线性）

**为了解决初始选取质心的问题**  （层次聚类和初始选取中心都有弊端）

使用两种比较好的方法来解决这个问题

+ 二分K均值

+ 后处理

后处理通过分裂簇和合并簇，实现减小SSE并且保持簇的个数不变

+ 分裂一个簇

通过分裂最大SSE的簇

+ 增加一个质心

选择离所有簇最远的点

+ 删除一个簇的质心

删除最小SSE的簇的质心，然后归为其他簇

+ 合并一个簇

合并质心最近的两个簇

#### 二分K均值

思想：不直接分为K个簇，而是先分为两个簇，然后从中选取一个继续分，循环直到K个簇

该方法不太受初始化问题的影响

二分K均值是层次（嵌套）的

##### 局限性

K均值算法具有一定局限性（簇的类型）k均值算法不适合非球形的簇，也不适合不同尺寸的簇

**由于簇的密度，簇的形状，簇的尺寸，这些簇的类型导致SSE目标函数不能很好聚类（K均值目标函数是最小化等尺寸等密度的球形簇）**



### 凝聚的层次聚类

过程是开始每一个点作为一个簇，然后根据某个参数来合并两个临近的簇，知道产生单个，包含所有点的簇。

明显，基于层次聚类，或者说是嵌套的，从原始样本到一个整体，逐渐凝聚的过程



### DBSCAN

DBSCAN基于密度的，划分的聚类

将点划分为核心点，边界点，噪声点

## 簇评估

非监督簇评估 **凝聚度和分离度**



#### 凝聚度和分离度基于图的观点

凝聚度：一个簇内的任意两点之间的邻近度之和

分离度：不同簇点与点之间邻近度之和

实际上说是边的权值，这个权值等于邻近度

#### 凝聚度和分离度基于原型的观点

凝聚度：点到原型（质心）的邻近度之和

分离度：两个度之间原型（质心）的邻近度

**总体簇有效性为个体簇有效性的加权和**

总体簇的度量就需要乘上一个权值，这个权值不是唯一的

**基于原型的凝聚度和基于图的凝聚度的关系**

对于SSE和欧几里得空间的点，可以证明，簇中逐队点的平均距离等于簇的SSE

SSE（Sum of Squared Errors）是用于衡量聚类结果的簇内方差或集中度的指标。	凝聚度

SSB（Sum of Squares Between）是用于衡量聚类结果的簇间离散度或分离度的指标。	分离度

SSE+SSB=定值

==轮廓系数==

越大越好

计算方法是基于对象（点）的，总体可以加和求均值

### 邻近度矩阵

也就是相似度矩阵，这个算法时间复杂度较大



**前面的方法都用于划分聚类接下来讲层次聚类的评估**



### 层次聚类评估

共性分类距离

#### 确定簇个数

例如通过二分k均值方法，然后计算SSE曲线，找到拐点即可

#### 聚类趋势度量

判断是否”存在“簇

Hopkins统计量

0.5表示没有，0或1表示有



1. **熵（Entropy）**：
   - 用于衡量分类或聚类结果的不确定性或混乱程度。
   - 熵值越高，表示结果中的混乱度越大，即分类或聚类结果不太一致。
   - 通常用于分类任务，计算方式为对每个类别的概率分布进行加权平均。
2. **纯度（Purity）**：
   - 用于衡量分类或聚类结果中的簇的纯度，即簇内的数据点是否都属于同一类别。
   - 纯度值越高，表示簇中的数据点越一致，即结果越好。
   - 通常用于聚类任务，计算方式为对每个簇中出现最频繁的类别的概率分布进行加权平均。
3. **精度（Precision）**：
   - 用于衡量分类或聚类结果中正类别（True Positive，TP）的比例。
   - 衡量了模型的预测结果中正类别的准确性。
   - 通常用于二分类问题，计算方式为 TP / (TP + False Positive)。
4. **召回率（Recall）**：
   - 用于衡量分类或聚类结果中正类别（True Positive，TP）的比例。
   - 衡量了模型能够正确检测到正类别的能力。
   - 通常用于二分类问题，计算方式为 TP / (TP + False Negative)。
5. **F度量（F-Measure）**：
   - 结合了精度和召回率，用于综合衡量分类或聚类结果的质量。
   - F度量是精度和召回率的调和平均值，通常用于平衡精确性和覆盖率。
   - 通常用于二分类问题，计算方式为 (2 * Precision * Recall) / (Precision + Recall)。



## 关联分析

**用于发现隐藏在大型数据集中有意义的联系**

**联系可以用关联规则或者频繁项集表示**



事务集（所有事务） T

项集（所有项） I

事务的宽度：事务中出现项的个数

支持度计数：项集在所有事务中出现的次数



关联规则是蕴含表达式，x->y  x，y是不相交的项集

**关联规则的强度可以用支持度和置信度度量**

支持度计算   x并y的支持度计数/总事务集的个数

置信度计算  x并y的支持度计数/x的支持度计数



==对于关联规则的解释应当小心，因为关联规则的推理并不存在因果关系，只是规则前件与后件的规律，并且有时需要专业知识才能对齐解释==



频繁项集，对于非频繁项集直接进行剪枝，减少计算量，决定是否频繁其实就看支持度

规则产生，从频繁项集中提取高置信度规则，得到强规则

#### 频繁项集的产生

格结构用于枚举所有可能的项集



降低产生频繁项集时间复杂度的方法

+ 通过先验原理减少候选项集的个数

+ 通过减少比较次数



### APRIORI算法



以下是Apriori算法的主要迭代步骤：

1. 初始迭代（k=1）：在这一轮中，首先扫描事务数据库以计算每个单个项的支持度，并找出频繁1项集。这一步通常被称为"1-项集生成"。

2. 后续迭代（k=2到Kmax）：在每一轮中，Apriori算法会使用频繁(k-1)项集生成候选k项集，然后扫描事务数据库以计算候选k项集的支持度，进一步筛选出频繁k项集。这些迭代步骤被称为"k-项集生成"和"k-项集剪枝"。

3. 终止迭代（k=Kmax+1）：Apriori算法的最后一轮迭代用于生成Kmax+1项集的候选项集。

总结起来，Apriori算法通过Kmax+1轮迭代来挖掘频繁项集，每一轮都处理特定长度的候选项集。这种迭代策略有助于减少计算的复杂性，因为在每一轮迭代中，只需考虑特定长度的项集，并且通过频繁项集的支持度来剪枝，避免了不必要的计算。这使得算法更高效，特别是在处理大规模事务数据库时。

**频繁项集产生**

候选集的产生

候选集的剪枝



hash树支持度计数

构建hash树，枚举事务，比较，目的是减少比较次数

**规则产生**

基于置信度的剪枝，如果a是b的子集，则a的支持度肯定更大，所以a的置信度就更小，如果b不满足置信度阙值，a一定也不满足





讲一下总体：首先为了从事务集提取规则，那就先需要找到出现次数多的项集，也就是频繁项集，要找到频繁项集，通过先遍历事务集得到全部项集，然后令为频繁1项集，接着从频繁1项集得到候选2项集（Fk-1× Fk-1），之后应该比较是否满足支持度阙值，这里复杂度很大，所以使用hash树减少比较次数，然后得到频繁k项集

在频繁k项集中，不需要再次扫描数据集，里面的置信度可以直接求得，但是这样会产生很多规则，所以对规则进行剪枝，根据子集的置信度一定小于超集的置信度，可以对低置信度进行剪枝，从而减少规则。



### 频繁项集的紧凑表示

#### 极大频繁项集

极大值（   

就是直接超集全是非频繁，自己是频繁项集



#### 闭频繁项集

闭频繁项集首先是频繁项集，然后满足闭的条件

闭：该项集的任意超集的支持度与自己都不相等，换句话说，超集的支持度计数一定更小

可以推出，对于非闭项集，它的支持度计算算法，等于其超集中最大的支持度计数



### FP-GROWTH

首先扫描一遍数据集，然后确定每个项的支持度计数

然后按照从高到低排序

然后构建FP树

FP树的规模与数据特点有关，相同前缀越多越小

还与项的排列有关



然后根据自底向上的方法，从最底下的，也就是项计数最低的，找到以它结尾的所有路径（每个结点都有指向相同项的指针，所以很好找）

假设项从高到低排序：a b c d e

然后找到频繁项集

具体方法

1. 找到e的所有路径，这些路径称为前缀路径
2. 然后判断e是否为频繁项集，把e的项的支持度计数相加，与最小支持度计数比较，如果更大则是
3. 判断e为是后，将e的前缀路径转化为条件FP树

​		条件FP树构造方法

  		将不包含e的路径上的项的支持度计数更改

​		 将e结点删除

​		 将支持度计数小于最小支持度计数的点删除

4. 然后使用e的条件FP树来解决，be，ce ，be，ae结尾的频繁项集的问题

5. 然后构建以be结尾的前缀路径，构建be结尾的条件FP树，然后判断是否频繁，递归操作



其实具体到实现可能有些问题，例如，怎么得到FP条件树



### 关联模式的评估

#### 客观兴趣度量

由于置信度和支持度有局限性，所以使用一些新的方式来评估关联模式

置信度忽略了规则后件，支持度可能忽略了一些有趣的但是支持度小的关联模式

+ 兴趣因子

提升度：提升度有多种定义，书上为规则的置信度/后件的支持度

兴趣因子：定义为规则的支持度/（前件的支持度乘后件的支持度）

+ 相关性分析

相关性分析的缺陷在于将同时出现和同时不出现看的同样重要

+ IS度量

根号兴趣因子乘规则支持度

等等一堆度量，总的来说可以分类对称性度量和非对称性度量

**客观度量的性质**

反演性：f00与f11 f10与f01对调，是否不变

零加性：添加f00，是否不变

缩放性：行或列等比缩放，是否不变



#### 倾斜支持度分布

大多数项具有较低或中等频率，但是少数项具有很高的频率

